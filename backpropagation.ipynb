{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical gradiend\n",
    "def numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4\n",
    "    grads = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.shape[0]):\n",
    "        temp = x[idx]\n",
    "\n",
    "        x[idx] = x + h\n",
    "        fx1 = f(x)\n",
    "\n",
    "        x[idx] = x - h\n",
    "        fx2 = f(x)\n",
    "\n",
    "        grads[idx] = (fx1 - fx2) / 2*h\n",
    "        x[idx] = temp\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    if x.ndim == 1:\n",
    "        return numerical_gradient_no_batch(f, x)\n",
    "    \n",
    "    grads = np.zeros_like(x)\n",
    "    for idx, x1 in enumerate(x):\n",
    "        grads[idx] = numerical_gradient_no_batch(f, x1)\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "def gradient_descent(f, init_x, lr, iter):\n",
    "    \n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for _ in iter:\n",
    "        grad = numerical_gradient(f, init_x)\n",
    "        x = x - lr * grad\n",
    "        x_history.append(x)\n",
    "    \n",
    "    return x, x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mullayer:\n",
    "    def __init__(self):\n",
    "        self.x1 = None\n",
    "        self.x2 = None\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "        return x1 * x2\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx1 = dout * self.x2\n",
    "        dx2 = dout * self.x1\n",
    "        \n",
    "        return dx1, dx2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addlayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twolayer:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.zeros((input_size, hidden_size))\n",
    "        self.params['b1'] = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.params['W2'] = np.zeros((hidden_size, output_size))\n",
    "        self.params['b2'] = np.zeros((1, output_size))\n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        x.reshape(1, x.shape[0])\n",
    "        q1 = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "\n",
    "        z1 = sigmoid(q1)\n",
    "        q2 = np.dot(z1, self.params['W2']) + self.params['b2']\n",
    "\n",
    "        y = softmax(q2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t): \n",
    "        y = predict(x) #이미 softmax 처리 되어있음\n",
    "        \n",
    "        return cross_entrophy(y, t)\n",
    "\n",
    "\n",
    "    def accuracy(self, x, t): \n",
    "        y = predict(x)\n",
    "        max_y = np.max(y, axis = 1)\n",
    "        max_t = np.max(t, axis = 1)\n",
    "\n",
    "        return np.sum(max_y == max_t) / x.shape[0]\n",
    "\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads  = {}\n",
    "        grads['W1'] = numercial_gradient(loss_W, self.params['W1'])\n",
    "        grads['W2'] = numercial_gradient(loss_W, self.params['W2'])\n",
    "        grads['W3'] = numercial_gradient(loss_W, self.params['W3'])\n",
    "        grads['W4'] = numercial_gradient(loss_W, self.params['W4'])\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "\n",
    "\n",
    "        #forward\n",
    "        a1 = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z1 = sigmoid(q1)\n",
    "        a2 = np.dot(z1, self.params['W2']) + self.params['b2']\n",
    "        y = softmax(q2)\n",
    "\n",
    "        W1 = self.params['W1']\n",
    "        W2 = self.params['W2']\n",
    "        b1 = self.params['b1']\n",
    "        b2 = self.params['b2']\n",
    "\n",
    "        #backward\n",
    "        batch_size = t.shape[0]\n",
    "        dy = (y-t) / batch_size\n",
    "\n",
    "        da2 = dy\n",
    "        db2 = np.sum(da2, axis = 0) \n",
    "\n",
    "        dz1 = np.dot(da2, W2.T)\n",
    "        dw2 = np.dot(z1.t, da2)\n",
    "\n",
    "        da1 = dz1 * z1 * (1-z1)\n",
    "\n",
    "        dx = np.dot(da1, W1.T)\n",
    "        dW1 = np.dot(x.T, da1)\n",
    "\n",
    "        db1 = np.sum(da1, axis = 0)\n",
    "        #저장도 해야함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    col_sum = np.sum(x, axis=1)\n",
    "    np.sum\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "\n",
    "def cross_entrophy(y, t): #y: softmax 처리 완료, t: one hot encoding일 수도/아닐수도\n",
    "    if y.ndim == 1:\n",
    "        y.reshape(1, y.reshape[0])\n",
    "        t.reshape(1, t.reshape[0])\n",
    "\n",
    "    if t.size == x.size:          #t가 one hot encoding 되어 있으면\n",
    "        flat_t = np.argmax(t, axis =1)\n",
    "\n",
    "    extract = y[np.rarange(y.shape[0], t)] + 1e-8\n",
    "    minus_log_col = -np.log(extract)\n",
    "    return np.sum(minus_log_col) /batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "mtrx = np.random.randn(4,3)\n",
    "print(mtrx.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change_into_one hot\n",
    "def one_hot(t):\n",
    "    mtrx = np.zeros(t.size, 10)\n",
    "\n",
    "    for idx, row in enumerate(mtrx):\n",
    "        row[mtrx[idx]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training process\n",
    "\n",
    "net1 = Twolayer(input_size= 1, hidden_size= 1, output_size= 1)\n",
    "\n",
    "iter_num = 3\n",
    "train_size = 3\n",
    "batch_size = 3\n",
    "lr = 3\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "for i in iter_num:\n",
    "    #batch 뽑기\n",
    "    batch_mask = np.random.rand(train_size, batch_size)\n",
    "\n",
    "    batch_x = train_x[batch_mask]\n",
    "    batch_t = train_t[batch_mask]\n",
    "    #각 batch별로 parameter update\n",
    "\n",
    "    grads = {}\n",
    "    grads = net1.gradient(batch_x, batch_t)\n",
    "\n",
    "    for keys in grads.keys():\n",
    "        params[keys] = params[keys] - lr * grads[keys]\n",
    "\n",
    "    #epoch 아닐때도 loss는 계속 append한다!\n",
    "    #loss는 현재 batch에서, acc는 전체에서\n",
    "    loss = net1.loss(batch_x, batch_t)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    \n",
    "    #epoch마다 한번씩 결과 보고\n",
    "    if i % iter_per_epoch % 0 == 0:\n",
    "\n",
    "        train_acc = net1.accuracy(train_x, train_t)\n",
    "        test_acc = net1.accuracy(test_x, test_t)\n",
    "\n",
    "        #append acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x>=0)\n",
    "        self.mask = mask\n",
    "        forward = x.copy()\n",
    "        result = forward[mask]\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout1 = dout[self.mask]\n",
    "\n",
    "        return dout1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid\n",
    "class sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / np.exp(-x)\n",
    "        self.out = out\n",
    "        #dout update\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        out = self.out\n",
    "        dx = dout* out * (1-out)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affine\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self, x):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two layer input\n",
    "class two_layer:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01): #여기서 input size는 feature의 개수\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.rand(input_size, hidden_size)\n",
    "        self.params['W2'] = np.random.rand(hidden_size, output_size)\n",
    "        self.params['b1'] = np.random.rand(1, hidden_size) #이렇게하면 의도와는 다르게 broadcasting되어 버림\n",
    "        self.params['b2'] = np.random.rand(1, output_size) #이렇게하면 의도와는 다르게 broadcasting되어 버림\n",
    "\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y_predict = self.predict(x)\n",
    "        CE_loss = cross_entrophy_loss(y_predict, t)\n",
    "\n",
    "        return CE_loss\n",
    "\n",
    "    def predict(self, X): \n",
    "        a1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        z1 = sigmoid(a1)\n",
    "        a2= np.dot(z1, self.params['W2']) + self.params['b2']\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        self.grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        self.grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        self.grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        self.grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "\n",
    "    def gradient(self, x, t): #backward\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_norm = x.shape[0]\n",
    "\n",
    "        #forward\n",
    "        a1 = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z1 = sigmoid(a1)\n",
    "        a2= np.dot(z1, self.params['W2']) + self.params['b2']\n",
    "        y = softmax(a2)\n",
    "\n",
    "        y = self.predict(x)\n",
    "        dy = (y-t) / batch_norm\n",
    "\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_backward(a1) * dz1  #처음에 여기 dz1안 곱했음\n",
    "        \n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis = 0)\n",
    "\n",
    "        return dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1/1+np.exp(-x)\n",
    "    \n",
    "def sigmoid_backward(x):\n",
    "    y = sigmoid(x)\n",
    "    return np.dot(y,(1-y))\n",
    "\n",
    "def softmax(x): \n",
    "    exp_x = np.exp(x)\n",
    "    exp_sum = np.sum(exp_x, axis = 1)\n",
    "\n",
    "    return x/exp_sum\n",
    "\n",
    "def cross_entrophy_loss(y, t):\n",
    "    \n",
    "    return -t/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient_no_batch(f, x):\n",
    "  h = 1e-4  # 0.0001\n",
    "  grad = np.zeros_like(x)  # an array with the same shape as 'x'\n",
    "\n",
    "  for idx in range(x.size):\n",
    "    tmp_val = x[idx]\n",
    "\n",
    "    # f(x + h)\n",
    "    x[idx] = tmp_val + h  #현재 x보다 아주 조금 옆에\n",
    "    fxh1 = f(x) #거기서의 f값\n",
    " \n",
    "    # f(x - h)\n",
    "    x[idx] = tmp_val - h \n",
    "    fxh2 = f(x)\n",
    "\n",
    "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "    x[idx] = tmp_val  # restore the original value\n",
    "\n",
    "  return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X): #X: multidimensional인 경우\n",
    "  if X.ndim == 1: #1 data example\n",
    "    return _numerical_gradient_no_batch(f, X)\n",
    "  else:\n",
    "    grad = np.zeros_like(X)\n",
    "\n",
    "    for idx, x in enumerate(X):  #i \n",
    "      grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'https://github.com/WegraLee/deep-learning-from-scratch/raw/master/dataset/'\n",
    "key_file = {\n",
    "  'train_img':'train-images-idx3-ubyte.gz',\n",
    "  'train_label':'train-labels-idx1-ubyte.gz',\n",
    "  'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "  'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = '/content'  # Colab base dir\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "  file_path = dataset_dir + \"/\" + file_name\n",
    "\n",
    "  if os.path.exists(file_path):\n",
    "      return\n",
    "\n",
    "  print(\"Downloading \" + file_name + \" ... \")\n",
    "  urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "  print(\"Done\")\n",
    "\n",
    "def download_mnist():\n",
    "  for v in key_file.values():\n",
    "    _download(v)\n",
    "\n",
    "def _load_label(file_name):\n",
    "  file_path = dataset_dir + \"/\" + file_name\n",
    "\n",
    "  print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "  with gzip.open(file_path, 'rb') as f:\n",
    "    labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "  print(\"Done\")\n",
    "\n",
    "  return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "  file_path = dataset_dir + \"/\" + file_name\n",
    "\n",
    "  print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "  with gzip.open(file_path, 'rb') as f:\n",
    "    data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "  data = data.reshape(-1, img_size)\n",
    "  print(\"Done\")\n",
    "\n",
    "  return data\n",
    "\n",
    "def _convert_numpy():\n",
    "  dataset = {}\n",
    "  dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "  dataset['train_label'] = _load_label(key_file['train_label'])\n",
    "  dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "  dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def init_mnist():\n",
    "  download_mnist()\n",
    "  dataset = _convert_numpy()\n",
    "  print(\"Creating pickle file ...\")\n",
    "  with open(save_file, 'wb') as f:\n",
    "    pickle.dump(dataset, f, -1)\n",
    "  print(\"Done!\")\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "  T = np.zeros((X.size, 10))\n",
    "  for idx, row in enumerate(T):\n",
    "    row[X[idx]] = 1\n",
    "\n",
    "  return T\n",
    "\n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "  \"\"\"Read MNIST\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  normalize : Image pixel values become 0.0~1.0\n",
    "  one_hot_label :\n",
    "    encodes labels as one hot vectores\n",
    "    an example of an one-hot : [0,0,1,0,0,0,0,0,0,0]\n",
    "  flatten : makes the images 1-D vectors\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  (Train Images, Train Labels), (Test Images, Test Images)\n",
    "  \"\"\"\n",
    "  if not os.path.exists(save_file):\n",
    "      init_mnist()\n",
    "\n",
    "  with open(save_file, 'rb') as f:\n",
    "      dataset = pickle.load(f)\n",
    "\n",
    "  if normalize:\n",
    "      for key in ('train_img', 'test_img'):\n",
    "          dataset[key] = dataset[key].astype(np.float32)\n",
    "          dataset[key] /= 255.0\n",
    "\n",
    "  if one_hot_label:\n",
    "      dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "      dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
    "\n",
    "  if not flatten:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "          dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "  return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/train-images-idx3-ubyte.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/munkijeong/Desktop/지스트 2023-2학기/3. 인공지능 경험랩/backpropagation.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# read datar\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m (x_train, t_train), (x_test, t_test) \u001b[39m=\u001b[39m load_mnist(normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, one_hot_label\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m network \u001b[39m=\u001b[39m TwoLayerNet(input_size\u001b[39m=\u001b[39m\u001b[39m784\u001b[39m, hidden_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m) \u001b[39m#we have 10 classes\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# hyper-parameters\u001b[39;00m\n",
      "\u001b[1;32m/Users/munkijeong/Desktop/지스트 2023-2학기/3. 인공지능 경험랩/backpropagation.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read MNIST\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m(Train Images, Train Labels), (Test Images, Test Images)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(save_file):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     init_mnist()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(save_file, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     dataset \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;32m/Users/munkijeong/Desktop/지스트 2023-2학기/3. 인공지능 경험랩/backpropagation.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_mnist\u001b[39m():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m   download_mnist()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m   dataset \u001b[39m=\u001b[39m _convert_numpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating pickle file ...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/munkijeong/Desktop/지스트 2023-2학기/3. 인공지능 경험랩/backpropagation.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_mnist\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m   \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m key_file\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     _download(v)\n",
      "\u001b[1;32m/Users/munkijeong/Desktop/지스트 2023-2학기/3. 인공지능 경험랩/backpropagation.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m file_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlretrieve(url_base \u001b[39m+\u001b[39;49m file_name, file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/munkijeong/Desktop/%EC%A7%80%EC%8A%A4%ED%8A%B8%202023-2%ED%95%99%EA%B8%B0/3.%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B2%BD%ED%97%98%EB%9E%A9/backpropagation.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:251\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# Handle temporary file setup.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m--> 251\u001b[0m     tfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     tfp \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mNamedTemporaryFile(delete\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train-images-idx3-ubyte.gz'"
     ]
    }
   ],
   "source": [
    "# read datar\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) \n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) #we have 10 classes\n",
    "\n",
    "# hyper-parameters\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# iteration per epoch\n",
    "iter_per_epoch = max(train_size / batch_size, 1) #reasoning: \n",
    "\n",
    "for i in range(iters_num):\n",
    "  # get minibatch\n",
    "  batch_mask = np.random.choice(train_size, batch_size) #1(0)부터 train_size까지 숫자 중 임의로 batch size만큼 뽑기\n",
    "  x_batch = x_train[batch_mask]\n",
    "  t_batch = t_train[batch_mask]\n",
    "\n",
    "  # gradient calculation\n",
    "  #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "  grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "  # update parameters\n",
    "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "  # record training process\n",
    "  loss = network.loss(x_batch, t_batch)\n",
    "  train_loss_list.append(loss)\n",
    "\n",
    "  # log accuracy at the end of each epoch\n",
    "  if i % iter_per_epoch == 0: #0-iters_num까지 계속 update되는데 그 과정에서 iter_per_epoch번마다 한번씩 성능을 확인함(이때를 epoch이라고 정의)\n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    print(f\"train acc, test acc | {train_acc:.4f}, {test_acc:.4f}\")\n",
    "\n",
    "# plot the result\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
